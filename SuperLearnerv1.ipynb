{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da19f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import statsmodels.api as sm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.optimize import minimize \n",
    "from scipy.optimize import nnls \n",
    "import pandas as pd\n",
    "# NOTE: THIS CODE ONLY WORKS FOR UNI-DIMENSIONAL OUTPUT (REGRESSION OR CLASSIFICATION)\n",
    "\n",
    "N = 1000\n",
    "output = 'cls'  # 'cls' for classification 'proba' for probabilities, 'reg' for regression\n",
    "k = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17817d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_classification(n_samples=N, n_features=8, n_clusters_per_class=4,\n",
    "                            n_informative=4, n_redundant=0, hypercube=False,\n",
    "                            random_state=0, shuffle=True, class_sep=0.1)\n",
    "\n",
    "def fn(x, A, b):\n",
    "    return np.linalg.norm(A.dot(x) - b)\n",
    "\n",
    "def combiner_solve(x, y):\n",
    "    # adapted from https://stackoverflow.com/questions/33385898/how-to-include-constraint-to-scipy-nnls-function-solution-so-that-it-sums-to-1/33388181\n",
    "    beta_0, rnorm = nnls(x,y)\n",
    "    cons = {'type': 'eq', 'fun': lambda x:  np.sum(x)-1}\n",
    "    bounds = [[0.0,None]]*x.shape[1]\n",
    "    minout = minimize(fn, beta_0, args=(x, y), method='SLSQP',bounds=bounds,constraints=cons)\n",
    "    beta = minout.x\n",
    "    return beta\n",
    "\n",
    "\n",
    "class SuperLearner(object):\n",
    "    def __init__(self, output, est_dict, k, standardized_outcome=False):\n",
    "        if standardized_outcome and (output=='proba' or output=='cls'):    \n",
    "            print('WARNING: Standardizing outcome for a classification task will cause problems...')\n",
    "        self.k = k  # number of cross validation folds\n",
    "        self.beta = None\n",
    "        self.output = output  # 'reg' for regression, 'proba' or 'cls' classification\n",
    "        self.trained_superlearner = None\n",
    "        self.est_dict = est_dict  # dictionary of learners/algos  \n",
    "        self.standardized_outcome = standardized_outcome\n",
    "        \n",
    "        self.x_std = None\n",
    "        self.x_mean = None\n",
    "        self.y_std = None\n",
    "        self.y_mean = None\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        x = x.values if isinstance(x, pd.DataFrame) else x\n",
    "        y = y.values[:,0] if isinstance(y, pd.DataFrame) else y\n",
    "      \n",
    "        # mean and std for full dataset (can be reused wth new data at prediction time)\n",
    "        self.x_std = x.std(0)\n",
    "        self.x_mean = x.mean(0)\n",
    "        \n",
    "        if self.standardized_outcome:\n",
    "            self.y_std = y.std(0)\n",
    "            self.y_mean = y.mean(0)\n",
    "\n",
    "        if (self.output == 'cls') or (self.output == 'proba'):\n",
    "            num_classes = np.unique(y)\n",
    "            if len(num_classes) == 2:\n",
    "                num_classes = 1\n",
    "            elif len(num_classes) > 2:\n",
    "                num_classes = len(num_classes)\n",
    "                self.output = 'cat'\n",
    "        else:\n",
    "            num_classes = 1\n",
    "            \n",
    "        kf = KFold(n_splits=self.k, shuffle=True, random_state=0)\n",
    "        \n",
    "        all_preds = np.zeros((len(y), num_classes, len(self.est_dict)))  # for test preds\n",
    "        \n",
    "        i = 0\n",
    "        for key in self.est_dict.keys():\n",
    "            print('Training estimator:', key)\n",
    "            \n",
    "            est = self.est_dict[key]\n",
    "            \n",
    "            if self.output == 'proba' or (self.output == 'cls'):\n",
    "                probs = []\n",
    "            preds = []\n",
    "            gts = []\n",
    "            \n",
    "            for train_index, test_index in kf.split(x):\n",
    "                x_train = x[train_index]\n",
    "                x_test =  x[test_index]\n",
    "                y_train = y[train_index]\n",
    "                y_test = y[test_index]\n",
    "\n",
    "                # per train/test fold means and standard deviations\n",
    "                x_std = x_train.std(0)\n",
    "                x_mean = x_train.mean(0)\n",
    "                x_train = (x_train - x_mean) / x_std\n",
    "                x_test = (x_test - x_mean) / x_std\n",
    "                \n",
    "                if self.standardized_outcome:\n",
    "                    y_std = y_train.std(0)\n",
    "                    y_mean = y_train.mean(0)\n",
    "                    y_train = (y_train - y_mean) / y_std\n",
    "                    y_test = (y_test - y_mean) / y_std\n",
    "                \n",
    "                if key == 'poly':\n",
    "                    est = LogisticRegression(C=1e2, max_iter=350) if (self.output == 'cls') or (\n",
    "                                self.output == 'proba') else LinearRegression()\n",
    "                    poly = PolynomialFeatures(2)\n",
    "                    x_train_poly = poly.fit_transform(x_train)\n",
    "                    x_test_poly = poly.fit_transform(x_test)\n",
    "\n",
    "                    est.fit(x_train_poly, y_train)                 \n",
    "                    \n",
    "                else:\n",
    "                    est.fit(x_train, y_train)\n",
    "                    \n",
    "                \n",
    "                if self.output == 'proba' or (self.output == 'cls'):\n",
    "                    p_robs = est.predict_proba(x_test_poly) if key == 'poly' else est.predict_proba(x_test)\n",
    "                    probs.append(p_robs)\n",
    "\n",
    "                p = est.predict(x_test_poly) if key == 'poly' else est.predict(x_test)\n",
    "                preds.append(p)\n",
    "                gts.append(y_test)\n",
    "                \n",
    "            preds = np.concatenate(preds)\n",
    "            if self.output == 'proba' or (self.output == 'cls'):\n",
    "                probs = np.concatenate(probs)\n",
    "            if num_classes == 1:\n",
    "                if self.output == 'proba' or (self.output == 'cls'):\n",
    "                    probs = probs[:, 1].reshape(-1, 1)\n",
    "                preds = preds.reshape(-1, 1)\n",
    "\n",
    "            gts = np.concatenate(gts)\n",
    "            \n",
    "            if (self.output == 'cls') or (self.output == 'proba'):\n",
    "                all_preds[:, :, i] = probs\n",
    "            elif self.output == 'reg' or self.output == 'cat':\n",
    "                all_preds[:, :, i] = preds\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        # estimate betas on test predictions\n",
    "        self.beta = combiner_solve(all_preds[:, 0, :], gts)  # all_preds is of shape [batch, categories, predictors]\n",
    "\n",
    "        # now train each estimator on full dataset\n",
    "\n",
    "        x = (x - self.x_mean) / self.x_std\n",
    "        if self.standardized_outcome:\n",
    "            y = (y-self.y_mean) / self.y_std\n",
    "            \n",
    "        for key in self.est_dict.keys():\n",
    "            print('Training estimator on full data:', key)\n",
    "            \n",
    "            est = self.est_dict[key]\n",
    "            \n",
    "            if key == 'poly':\n",
    "                est = LogisticRegression(C=1e2, max_iter=350) if (self.output == 'cls') or (\n",
    "                            self.output == 'proba') else LinearRegression()\n",
    "                poly = PolynomialFeatures(2)\n",
    "                x_poly = poly.fit_transform(x)\n",
    "\n",
    "                est.fit(x_poly, y)                    \n",
    "                    \n",
    "            else:\n",
    "                est.fit(x, y)\n",
    "                    \n",
    "            self.est_dict[key] = est\n",
    "            \n",
    "            \n",
    "    def predict(self, x):\n",
    "\n",
    "        x_ = (x - self.x_mean) / self.x_std\n",
    "\n",
    "        all_preds = np.zeros((len(x), len(self.est_dict)))\n",
    "        i = 0\n",
    "\n",
    "        for key in self.est_dict.keys():\n",
    "            est = self.est_dict[key]\n",
    "            if key == 'poly':\n",
    "                poly = PolynomialFeatures(2)\n",
    "                x_scaled = poly.fit_transform(x_)\n",
    "\n",
    "            if (self.output == 'cls') or self.output == 'proba':\n",
    "                preds = est.predict_proba(x_)[:, 1] if key != 'poly' else est.predict_proba(x_scaled)[:, 1]\n",
    "            else:\n",
    "                preds = est.predict(x_) if key != 'poly' else est.predict(x_scaled)\n",
    "   \n",
    "\n",
    "            all_preds[:, i] = preds\n",
    "\n",
    "            i += 1\n",
    "        \n",
    "        weighted_preds = np.dot(all_preds, self.beta)\n",
    "        weighted_preds = weighted_preds.reshape(-1, 1)\n",
    "        if self.standardized_outcome:\n",
    "            weighted_preds = (weighted_preds * self.y_std) + self.y_mean\n",
    "        return weighted_preds\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97607b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5f60b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training estimator: LR\n",
      "Training estimator on full data: LR\n",
      "[1.] 0.552\n"
     ]
    }
   ],
   "source": [
    "est_dict = {'LR':LogisticRegression()}\n",
    "\n",
    "SL = SuperLearner(output='cls', est_dict=est_dict, k=k, standardized_outcome=False)\n",
    "SL.fit(x, y)\n",
    "preds = SL.predict(x)\n",
    "preds = np.round(preds)\n",
    "print(SL.beta, balanced_accuracy_score(y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49d516d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training estimator: LR\n",
      "Training estimator: SVC\n",
      "Training estimator on full data: LR\n",
      "Training estimator on full data: SVC\n",
      "[8.10983225e-17 1.00000000e+00] 0.783\n"
     ]
    }
   ],
   "source": [
    "\n",
    "est_dict = {'LR':LogisticRegression(), 'SVC':SVC(probability=True)}\n",
    "SL = SuperLearner(output='cls', est_dict=est_dict, k=k, standardized_outcome=False)\n",
    "SL.fit(x, y)\n",
    "preds = SL.predict(x)\n",
    "preds = np.round(preds)\n",
    "print(SL.beta, balanced_accuracy_score(y, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76659725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training estimator: LR\n",
      "Training estimator: SVC\n",
      "Training estimator: RF\n",
      "Training estimator on full data: LR\n",
      "Training estimator on full data: SVC\n",
      "Training estimator on full data: RF\n",
      "[2.56895796e-16 8.19535772e-01 1.80464228e-01] 0.842\n"
     ]
    }
   ],
   "source": [
    "\n",
    "est_dict = {'LR':LogisticRegression(), 'SVC':SVC(probability=True), 'RF':RandomForestClassifier()}\n",
    "SL = SuperLearner(output='cls', est_dict=est_dict, k=k, standardized_outcome=False)\n",
    "SL.fit(x, y)\n",
    "preds = SL.predict(x)\n",
    "preds = np.round(preds)\n",
    "print(SL.beta, balanced_accuracy_score(y, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fd2832bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b98684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43eb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28ae2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20406d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "46879ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1be129b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445812f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6219327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
